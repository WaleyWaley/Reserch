{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "586dde3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename is 0.15_Speed_withoutOB.csv\n",
      "正在构建图数据 (只需一次)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 16:57:20,785] A new study created in memory with name: no-name-cec9c37e-a458-4740-a673-d5f9566808df\n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1760641939026789 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始 Optuna 自动调参...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-07 17:09:19,389] Trial 0 finished with value: 0.01145548671907322 and parameters: {'gnn_hidden_dim': 96, 'gnn_heads': 8, 'dropout': 0.1760641939026789, 'lstm_hidden': 28, 'lstm_num_layers': 1, 'gnn_num_layers': 3, 'kan_middle_dim': 176, 'lr_phase1': 0.00018973502509167045, 'weight_decay': 3.1897671798933224e-05, 'kan_reg_weight': 0.0003285476682340779, 'lambda_vel': 0.0012345841886109488, 'lr_phase2': 1.7931055964301815e-05, 'smooth_weight': 0.0001402689433861436, 'consistency_weight': 0.0007751104912733603}. Best is trial 0 with value: 0.01145548671907322.\n",
      "[I 2025-12-07 17:12:40,921] Trial 1 finished with value: 0.006127012830058282 and parameters: {'gnn_hidden_dim': 64, 'gnn_heads': 8, 'dropout': 0.06229604281983908, 'lstm_hidden': 28, 'lstm_num_layers': 2, 'gnn_num_layers': 2, 'kan_middle_dim': 96, 'lr_phase1': 0.0014859238111918848, 'weight_decay': 7.548720320713535e-05, 'kan_reg_weight': 0.00024844089388430593, 'lambda_vel': 0.1583765656505329, 'lr_phase2': 0.0006400005177155445, 'smooth_weight': 0.0003430466535430947, 'consistency_weight': 5.237583721524675e-05}. Best is trial 1 with value: 0.006127012830058282.\n",
      "[I 2025-12-07 17:15:27,461] Trial 2 finished with value: 0.007003090238537301 and parameters: {'gnn_hidden_dim': 32, 'gnn_heads': 4, 'dropout': 0.25521529967531953, 'lstm_hidden': 96, 'lstm_num_layers': 3, 'gnn_num_layers': 1, 'kan_middle_dim': 176, 'lr_phase1': 0.001387820913265279, 'weight_decay': 6.270787650283458e-05, 'kan_reg_weight': 0.00018173511120578366, 'lambda_vel': 0.08890101485494409, 'lr_phase2': 0.0004537009936147693, 'smooth_weight': 0.0001632932464623051, 'consistency_weight': 0.0004433202638002497}. Best is trial 1 with value: 0.006127012830058282.\n",
      "[I 2025-12-07 17:20:10,902] Trial 3 finished with value: 0.006252466944384982 and parameters: {'gnn_hidden_dim': 32, 'gnn_heads': 8, 'dropout': 0.3132173462129307, 'lstm_hidden': 32, 'lstm_num_layers': 2, 'gnn_num_layers': 3, 'kan_middle_dim': 32, 'lr_phase1': 0.004946141144084335, 'weight_decay': 0.000299539476472301, 'kan_reg_weight': 1.2862129419503473e-05, 'lambda_vel': 0.002663141011304553, 'lr_phase2': 0.00043527508873880065, 'smooth_weight': 0.00021211209736996922, 'consistency_weight': 5.812413431997219e-05}. Best is trial 1 with value: 0.006127012830058282.\n",
      "[I 2025-12-07 17:23:48,145] Trial 4 finished with value: 0.004869744364193387 and parameters: {'gnn_hidden_dim': 32, 'gnn_heads': 2, 'dropout': 0.046710104753555415, 'lstm_hidden': 28, 'lstm_num_layers': 2, 'gnn_num_layers': 1, 'kan_middle_dim': 64, 'lr_phase1': 0.00126510540241409, 'weight_decay': 0.0007009150792058413, 'kan_reg_weight': 0.00015411768976069092, 'lambda_vel': 0.10518862621134535, 'lr_phase2': 0.0005536326671639646, 'smooth_weight': 0.0004108665996383012, 'consistency_weight': 0.00015093421063778738}. Best is trial 4 with value: 0.004869744364193387.\n",
      "[I 2025-12-07 17:27:04,113] Trial 5 finished with value: 0.006143021206794815 and parameters: {'gnn_hidden_dim': 128, 'gnn_heads': 2, 'dropout': 0.22813451709600968, 'lstm_hidden': 64, 'lstm_num_layers': 3, 'gnn_num_layers': 2, 'kan_middle_dim': 192, 'lr_phase1': 0.004936503012133183, 'weight_decay': 0.000647158175238376, 'kan_reg_weight': 3.0389166336628705e-05, 'lambda_vel': 0.002596649543405293, 'lr_phase2': 0.0006201635614525664, 'smooth_weight': 0.0002915260908474258, 'consistency_weight': 1.167102255203796e-05}. Best is trial 4 with value: 0.004869744364193387.\n",
      "[I 2025-12-07 17:27:56,316] Trial 6 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2526254322445516 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:28:48,527] Trial 7 pruned. \n",
      "[I 2025-12-07 17:29:34,552] Trial 8 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2771497285263604 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:30:23,976] Trial 9 pruned. \n",
      "[I 2025-12-07 17:31:10,978] Trial 10 pruned. \n",
      "[I 2025-12-07 17:32:15,266] Trial 11 pruned. \n",
      "[I 2025-12-07 17:33:17,779] Trial 12 pruned. \n",
      "[I 2025-12-07 17:36:59,686] Trial 13 finished with value: 0.00563095707911998 and parameters: {'gnn_hidden_dim': 64, 'gnn_heads': 2, 'dropout': 0.12548512065649658, 'lstm_hidden': 28, 'lstm_num_layers': 2, 'gnn_num_layers': 1, 'kan_middle_dim': 112, 'lr_phase1': 0.0030015405846099425, 'weight_decay': 3.66912997533076e-05, 'kan_reg_weight': 0.00012584280133056877, 'lambda_vel': 0.2330421172452127, 'lr_phase2': 7.352751364170626e-05, 'smooth_weight': 8.623678123128772e-05, 'consistency_weight': 0.0001730378618072401}. Best is trial 4 with value: 0.004869744364193387.\n",
      "[I 2025-12-07 17:38:03,728] Trial 14 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4011055079602454 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:40:26,879] Trial 15 finished with value: 0.004622921633364802 and parameters: {'gnn_hidden_dim': 32, 'gnn_heads': 2, 'dropout': 0.4011055079602454, 'lstm_hidden': 28, 'lstm_num_layers': 1, 'gnn_num_layers': 1, 'kan_middle_dim': 128, 'lr_phase1': 0.002739081513013747, 'weight_decay': 3.4554602832872516e-05, 'kan_reg_weight': 4.17031854750352e-05, 'lambda_vel': 0.029230732276031263, 'lr_phase2': 3.7927820632194276e-05, 'smooth_weight': 1.0252192225568273e-05, 'consistency_weight': 0.00033433718376037804}. Best is trial 15 with value: 0.004622921633364802.\n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4487405595496786 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:41:11,807] Trial 16 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.49113333213429666 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:41:56,893] Trial 17 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3821317236151082 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:42:41,684] Trial 18 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4096907997475507 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:43:36,221] Trial 19 pruned. \n",
      "[I 2025-12-07 17:44:29,853] Trial 20 pruned. \n",
      "[I 2025-12-07 17:45:14,499] Trial 21 pruned. \n",
      "[I 2025-12-07 17:45:59,485] Trial 22 pruned. \n",
      "[I 2025-12-07 17:48:37,139] Trial 23 finished with value: 0.00615529872646386 and parameters: {'gnn_hidden_dim': 32, 'gnn_heads': 2, 'dropout': 0.05269389422300666, 'lstm_hidden': 28, 'lstm_num_layers': 3, 'gnn_num_layers': 1, 'kan_middle_dim': 64, 'lr_phase1': 0.0035673942681739104, 'weight_decay': 1.9873775382305286e-05, 'kan_reg_weight': 0.0004036238712528776, 'lambda_vel': 0.10916536498132498, 'lr_phase2': 4.44351164099211e-05, 'smooth_weight': 3.535751385638615e-05, 'consistency_weight': 0.0004867846479399522}. Best is trial 15 with value: 0.004622921633364802.\n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.16698129554063895 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:51:22,070] Trial 24 pruned. \n",
      "[I 2025-12-07 17:52:10,675] Trial 25 pruned. \n",
      "[I 2025-12-07 17:52:55,226] Trial 26 pruned. \n",
      "[I 2025-12-07 17:53:48,746] Trial 27 pruned. \n",
      "c:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4978973045795605 and num_layers=1\n",
      "  warnings.warn(\n",
      "[I 2025-12-07 17:54:32,926] Trial 28 pruned. \n",
      "[I 2025-12-07 17:55:34,874] Trial 29 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Optuna 搜索完成！\n",
      "==================================================\n",
      "Best trial:\n",
      "  Value (Best Val Loss): 0.004622921633364802\n",
      "  Params: \n",
      "    gnn_hidden_dim: 32\n",
      "    gnn_heads: 2\n",
      "    dropout: 0.4011055079602454\n",
      "    lstm_hidden: 28\n",
      "    lstm_num_layers: 1\n",
      "    gnn_num_layers: 1\n",
      "    kan_middle_dim: 128\n",
      "    lr_phase1: 0.002739081513013747\n",
      "    weight_decay: 3.4554602832872516e-05\n",
      "    kan_reg_weight: 4.17031854750352e-05\n",
      "    lambda_vel: 0.029230732276031263\n",
      "    lr_phase2: 3.7927820632194276e-05\n",
      "    smooth_weight: 1.0252192225568273e-05\n",
      "    consistency_weight: 0.00033433718376037804\n",
      "最佳参数已保存至 optuna_best_params.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 394\u001b[39m\n\u001b[32m    392\u001b[39m best_params_df.to_csv(\u001b[33m\"\u001b[39m\u001b[33moptuna_best_params.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    393\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m最佳参数已保存至 optuna_best_params.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m \u001b[43moptuna\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m optuna.visualization.plot_param_importances(study).show() \u001b[38;5;66;03m# 查看哪个参数最重要\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\plotly\\basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\plotly\\io\\_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna  # 【新增】导入 Optuna\n",
    "\n",
    "# ==============================================================================\n",
    "#                      【自定义模块导入】 (保持不变)\n",
    "# ==============================================================================\n",
    "# 请确保这些文件在您的路径下\n",
    "from loss_function.GNN_LSTM_with_Attention_V2 import GNN_LSTKAN_with_Attention_v2, GNNEncoder\n",
    "from Slide_Window_and_Graph import create_graph_list_from_df\n",
    "from kan_improved import KAN\n",
    "from loss_function.custom_lossess_v4 import jerk_smoothness_loss_v2, velocity_heading_consistency_loss_v2\n",
    "\n",
    "# ==============================================================================\n",
    "#                      全局设置与数据准备\n",
    "# ==============================================================================\n",
    "# 将数据加载放在全局，避免每次 Optuna 尝试时重复加载\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# filesname = \"Cheku.csv\" # 或者根据您的实际变量名\n",
    "\n",
    "# --- 1. 数据加载 ---\n",
    "# 假设数据已经存在，这里保留您的读取逻辑\n",
    "# 请根据实际情况取消注释或修改路径\n",
    "# try:\n",
    "#     train_data = pd.read_csv(r'Datasets/x_train.csv')\n",
    "#     val_data = pd.read_csv(r'Datasets/x_val.csv')\n",
    "#     test_data = pd.read_csv(r'Datasets/x_test.csv')\n",
    "#     print(\"数据加载成功！\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"【警告】未找到数据集文件，请检查路径。代码将无法运行数据部分。\")\n",
    "#     # 为了演示代码结构，这里模拟数据 (仅用于调试，实际运行时请删除)\n",
    "#     # train_data = pd.DataFrame(np.random.rand(100, 10), columns=[f'col_{i}' for i in range(10)])\n",
    "#     # val_data = pd.DataFrame(np.random.rand(20, 10), columns=[f'col_{i}' for i in range(10)])\n",
    "#     # test_data = pd.DataFrame(np.random.rand(20, 10), columns=[f'col_{i}' for i in range(10)])\n",
    "#     pass\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "direct = \"Datasets/\"\n",
    "filesname = \"0.15_Speed_withoutOB.csv\"\n",
    "print(f\"filename is {filesname}\")\n",
    "data = pd.read_csv(direct + filesname)\n",
    "# 2. 定义分割比例\n",
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "# # 剩下的 10% 将作为测试集\n",
    "\n",
    "# # 3. 计算分割点索引 (按时间顺序)\n",
    "data_size = len(data)\n",
    "train_end_idx = int(train_ratio * data_size)\n",
    "val_end_idx = train_end_idx + int(val_ratio * data_size)\n",
    "\n",
    "# # # 4. 使用 .iloc 分割 DataFrame\n",
    "train_data = data.iloc[:train_end_idx]\n",
    "val_data = data.iloc[train_end_idx:val_end_idx]\n",
    "test_data = data.iloc[val_end_idx:]\n",
    "\n",
    "# --- 2. 特征工程 ---\n",
    "column_names = train_data.drop(['timestamp','x_coord', 'y_coord'], axis=1).columns.tolist()\n",
    "coord_cols = ['x_coord', 'y_coord']\n",
    "wifi_features = [col for col in column_names if any(sensor in col for sensor in [\"RSSI\",\"distance\",\"rot\"])]\n",
    "imu_features = [col for col in column_names if any(sensor in col for sensor in [\"accelerometer\", \"gyroscope\",\"magnetometer\"])]\n",
    "\n",
    "scaler_wifi = StandardScaler().fit(train_data[wifi_features])\n",
    "scaler_imu = StandardScaler().fit(train_data[imu_features])\n",
    "scaler_coords = StandardScaler().fit(train_data[coord_cols])\n",
    "\n",
    "train_df_scaled = train_data.copy()\n",
    "val_df_scaled = val_data.copy()\n",
    "test_df_scaled = test_data.copy()\n",
    "\n",
    "for df in [train_df_scaled, val_df_scaled, test_df_scaled]:\n",
    "    df[wifi_features] = scaler_wifi.transform(df[wifi_features])\n",
    "    df[imu_features] = scaler_imu.transform(df[imu_features])\n",
    "    df[coord_cols] = scaler_coords.transform(df[coord_cols])\n",
    "\n",
    "# --- 3. 图构建 ---\n",
    "windows_size = 60\n",
    "future_steps = 2\n",
    "future_radius = 10\n",
    "past_radius = 20\n",
    "batch_size = 128  # 这个也可以作为超参数搜索，但通常固定为 64 或 128\n",
    "\n",
    "print(\"正在构建图数据 (只需一次)...\")\n",
    "train_data_list = create_graph_list_from_df(train_df_scaled, wifi_features, imu_features, windows_size, future_steps, 'cpu', future_radius, past_radius)\n",
    "val_data_list = create_graph_list_from_df(val_df_scaled, wifi_features, imu_features, windows_size, future_steps, 'cpu', future_radius, past_radius)\n",
    "# test_data_list 在 Optuna 搜索时不需要，最后评估才用\n",
    "\n",
    "train_loader = DataLoader(train_data_list, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data_list, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#al 剪枝)\n",
    "# # ==============================================================================\n",
    "# \n",
    "# def train_phase_1_optuna(model, train_loader, val_loader, lr, weight_decay, epochs, patience, kan_reg_weight, lambda_vel, trial):\n",
    "#     \"\"\"\n",
    "#     第一阶段训练 - 适配 Optuna\n",
    "#     \"\"\"\n",
    "#     criterion = nn.HuberLoss()\n",
    "#     optimizer = optim.AdamW(model.parameters(),)\"r=lr, weight_decay=weight_decay)\n",
    "#     scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "#     \n",
    "#     best_val_l 全局维度定义\n",
    "wifi_feat_dim = len(wifi_features)\n",
    "imu_feat_dim = len(imu_features)\n",
    "kan_output_dim = future_steps * 2\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#                      修正后的训练函数 (解决 AttributeError)\n",
    "# ==============================================================================\n",
    "\n",
    "def train_phase_1_optuna(model, train_loader, val_loader, lr, weight_decay, epochs, patience, kan_reg_weight, lambda_vel, trial):\n",
    "    \"\"\"\n",
    "    第一阶段训练 - 修正版\n",
    "    返回: best_model_state, best_val_loss, actual_epochs_run\n",
    "    \"\"\"\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    actual_epochs_run = 0 # 记录实际运行了多少轮\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        actual_epochs_run = epoch + 1 # 更新当前轮数\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            main_pred, _, _ = model(batch)\n",
    "            true_seq = batch.y.view(batch.num_graphs, future_steps, 2)\n",
    "            \n",
    "            loss_coord = criterion(main_pred, true_seq)\n",
    "            pred_vel = (main_pred[:, 1:] - main_pred[:, :-1])\n",
    "            true_vel = (true_seq[:, 1:] - true_seq[:, :-1])\n",
    "            loss_vel = criterion(pred_vel, true_vel)\n",
    "            \n",
    "            reg_loss = model.regularization_loss()\n",
    "            total_loss = loss_coord + lambda_vel * loss_vel + kan_reg_weight * reg_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_accum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred_seq, _, _ = model(batch)\n",
    "                true_seq = batch.y.view(batch.num_graphs, future_steps, 2)\n",
    "                \n",
    "                loss_coord = criterion(pred_seq, true_seq)\n",
    "                pred_vel = (pred_seq[:, 1:] - pred_seq[:, :-1])\n",
    "                true_vel = (true_seq[:, 1:] - true_seq[:, :-1])\n",
    "                loss_vel = criterion(pred_vel, true_vel)\n",
    "                \n",
    "                val_loss_accum += (loss_coord + lambda_vel * loss_vel).item()\n",
    "        \n",
    "        avg_val_loss = val_loss_accum / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # --- Optuna Pruning ---\n",
    "        trial.report(avg_val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_model_state = model.state_dict() # 保存权重副本\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "            \n",
    "    # 返回实际运行的轮数，供第二阶段接续\n",
    "    return best_model_state, best_val_loss, actual_epochs_run\n",
    "\n",
    "def train_phase_2_optuna(model, train_loader, val_loader, lr, weight_decay, epochs, patience, smooth_weight, consistency_weight, lambda_vel, trial, start_step):\n",
    "    \"\"\"\n",
    "    第二阶段训练 - 修正版\n",
    "    新增参数: start_step (来自第一阶段的轮数)\n",
    "    \"\"\"\n",
    "    criterion = nn.HuberLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            main_pred, _, _ = model(batch)\n",
    "            true_seq = batch.y.view(batch.num_graphs, future_steps, 2)\n",
    "            \n",
    "            loss_coord = criterion(main_pred, true_seq)\n",
    "            pred_vel = (main_pred[:, 1:] - main_pred[:, :-1])\n",
    "            true_vel = (true_seq[:, 1:] - true_seq[:, :-1])\n",
    "            loss_vel = criterion(pred_vel, true_vel)\n",
    "            \n",
    "            base_loss = loss_coord + lambda_vel * loss_vel\n",
    "            smooth_loss = jerk_smoothness_loss_v2(main_pred)\n",
    "            consistency_loss = velocity_heading_consistency_loss_v2(main_pred)\n",
    "            \n",
    "            total_loss = base_loss + smooth_weight * smooth_loss + consistency_weight * consistency_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss_accum = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                pred_seq, _, _ = model(batch)\n",
    "                true_seq = batch.y.view(batch.num_graphs, future_steps, 2)\n",
    "                \n",
    "                loss_coord = criterion(pred_seq, true_seq)\n",
    "                pred_vel = (pred_seq[:, 1:] - pred_seq[:, :-1])\n",
    "                true_vel = (true_seq[:, 1:] - true_seq[:, :-1])\n",
    "                loss_vel = criterion(pred_vel, true_vel)\n",
    "                \n",
    "                val_loss_accum += (loss_coord + lambda_vel * loss_vel).item()\n",
    "        \n",
    "        avg_val_loss = val_loss_accum / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Optuna Reporting (接续第一阶段的 step)\n",
    "        current_total_step = start_step + epoch\n",
    "        trial.report(avg_val_loss, current_total_step)\n",
    "        \n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "            \n",
    "    return best_val_loss\n",
    "\n",
    "# ==============================================================================\n",
    "#                      Optuna Objective Function (修正版)\n",
    "# ==============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    # --- 1. 采样超参数 ---\n",
    "    # 模型架构参数\n",
    "    gnn_hidden = trial.suggest_categorical(\"gnn_hidden_dim\", [32, 64, 96, 128])\n",
    "    gnn_heads = trial.suggest_categorical(\"gnn_heads\", [2, 4, 8])\n",
    "    dropout = trial.suggest_float(\"dropout\",0, 0.5)\n",
    "    \n",
    "    # 【新增】LSTM 相关的搜索参数\n",
    "    lstm_hidden = trial.suggest_categorical(\"lstm_hidden\", [32, 64, 96,28])\n",
    "    lstm_num_layers = trial.suggest_int(\"lstm_num_layers\", 1, 3) # 搜索 1, 2, 3 层\n",
    "    gnn_num_layers = trial.suggest_int(\"gnn_num_layers\", 1, 3)\n",
    "    kan_middle_dim = trial.suggest_int(\"kan_middle_dim\", 32, 256, step=16)\n",
    "    \n",
    "    # Phase 1 参数\n",
    "    lr_p1 = trial.suggest_float(\"lr_phase1\", 1e-4, 5e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n",
    "    kan_reg_w = trial.suggest_float(\"kan_reg_weight\", 1e-5, 1e-3, log=True)\n",
    "    lambda_vel = trial.suggest_float(\"lambda_vel\",0.001, 2, log=True)\n",
    "    \n",
    "    # Phase 2 参数\n",
    "    lr_p2 = trial.suggest_float(\"lr_phase2\", 1e-5, 1e-3, log=True) \n",
    "    smooth_w = trial.suggest_float(\"smooth_weight\", 1e-5, 1e-3, log=True)\n",
    "    consistency_w = trial.suggest_float(\"consistency_weight\", 1e-5, 1e-3, log=True)\n",
    "\n",
    "    # --- 2. 实例化模型 ---\n",
    "    gnn_encoder = GNNEncoder(\n",
    "        wifi_input_dim=wifi_feat_dim,\n",
    "        imu_input_dim=imu_feat_dim,\n",
    "        hidden_dim=gnn_hidden,\n",
    "        windows_size=windows_size,\n",
    "        num_layers=gnn_num_layers,\n",
    "        heads=gnn_heads,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "    kan_layers_config = [gnn_hidden, kan_middle_dim, 16, kan_output_dim]\n",
    "    \n",
    "    # 【注意】这里假设您的 KAN 类支持 lstm_num_layers 参数\n",
    "    # 如果您的 KAN 类中 LSTM 层数参数名是 num_layers，请相应修改\n",
    "    kan_predictor = KAN(\n",
    "        layers_hidden=kan_layers_config,\n",
    "        use_lstm=True,\n",
    "        lstm_hidden=lstm_hidden,\n",
    "        \n",
    "        # 【新增】传入搜索到的 LSTM 层数\n",
    "        # 请检查您的 kan_improved.py 文件，确认控制 LSTM 层数的参数名是否为 lstm_num_layers\n",
    "        # 如果您的代码中是用 num_layers 控制 LSTM，则写成 num_layers=lstm_num_layers\n",
    "        dropout_rate=dropout,\n",
    "        num_layers=lstm_num_layers, # 这里保留原有的 num_layers (假设它是控制 KAN 结构的层数)\n",
    "        grid_size=5,\n",
    "        spline_order=3,\n",
    "    ).to(device)\n",
    "\n",
    "    model = GNN_LSTKAN_with_Attention_v2(\n",
    "        gnn_encoder=gnn_encoder,\n",
    "        kan_predictor=kan_predictor,\n",
    "        future_steps=future_steps\n",
    "    ).to(device)\n",
    "\n",
    "    # --- 3. 执行 Phase 1 训练 ---\n",
    "    p1_epochs = 20\n",
    "    p1_patience = 15\n",
    "    \n",
    "    best_state_p1, _, epochs_run_p1 = train_phase_1_optuna(\n",
    "        model, train_loader, val_loader, \n",
    "        lr=lr_p1, weight_decay=weight_decay, \n",
    "        epochs=p1_epochs, patience=p1_patience, \n",
    "        kan_reg_weight=kan_reg_w, lambda_vel=lambda_vel,\n",
    "        trial=trial\n",
    "    )\n",
    "    \n",
    "    if best_state_p1 is None:\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    # --- 4. 执行 Phase 2 训练 ---\n",
    "    model.load_state_dict(best_state_p1)\n",
    "    \n",
    "    p2_epochs = 100\n",
    "    p2_patience = 15\n",
    "    \n",
    "    final_val_loss = train_phase_2_optuna(\n",
    "        model, train_loader, val_loader,\n",
    "        lr=lr_p2, weight_decay=weight_decay,\n",
    "        epochs=p2_epochs, patience=p2_patience,\n",
    "        smooth_weight=smooth_w, consistency_weight=consistency_w,\n",
    "        lambda_vel=lambda_vel, trial=trial,\n",
    "        start_step=epochs_run_p1 \n",
    "    )\n",
    "    \n",
    "    return final_val_loss\n",
    "# ==============================================================================\n",
    "#                      主程序入口\n",
    "# ==============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建 Study\n",
    "    # direction=\"minimize\" 表示我们要最小化 loss\n",
    "    # pruner=optuna.pruners.MedianPruner() 表示如果某次实验结果比之前实验的中位数还差，就停止\n",
    "    study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner(n_warmup_steps=10))\n",
    "    \n",
    "    print(\"开始 Optuna 自动调参...\")\n",
    "    # n_trials=50 表示总共尝试 50 组参数组合\n",
    "    study.optimize(objective, n_trials=30)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Optuna 搜索完成！\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(f\"  Value (Best Val Loss): {trial.value}\")\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "    # =================================================================\n",
    "    #      (可选) 使用搜索到的最佳参数重新训练最终模型并保存\n",
    "    # =================================================================\n",
    "    # 如果您想直接跑出最终结果，可以在这里解析 trial.params，\n",
    "    # 然后用这些参数重新实例化模型并调用您原始的 train_phase_1 和 train_phase_2 函数。\n",
    "    \n",
    "    # 简单的保存最佳参数到文件\n",
    "    best_params_df = pd.DataFrame([trial.params])\n",
    "    best_params_df.to_csv(\"optuna_best_params.csv\", index=False)\n",
    "    print(\"最佳参数已保存至 optuna_best_params.csv\")\n",
    "    optuna.visualization.plot_optimization_history(study).show()\n",
    "    optuna.visualization.plot_param_importances(study).show() # 查看哪个参数最重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14338dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\optuna\\visualization\\_plotly_imports.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m try_import() \u001b[38;5;28;01mas\u001b[39;00m _imports:\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m plotly_version\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'plotly'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43moptuna\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvisualization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplot_optimization_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m)\u001b[49m.show()\n\u001b[32m      3\u001b[39m optuna.visualization.plot_param_importances(study).show() \u001b[38;5;66;03m# 查看哪个参数最重要\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\optuna\\visualization\\_optimization_history.py:200\u001b[39m, in \u001b[36mplot_optimization_history\u001b[39m\u001b[34m(study, target, target_name, error_bar)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_optimization_history\u001b[39m(\n\u001b[32m    173\u001b[39m     study: Study | Sequence[Study],\n\u001b[32m    174\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    177\u001b[39m     error_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    178\u001b[39m ) -> \u001b[33m\"\u001b[39m\u001b[33mgo.Figure\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    179\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Plot optimization history of all trials in a study.\u001b[39;00m\n\u001b[32m    180\u001b[39m \n\u001b[32m    181\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m \u001b[33;03m        A :class:`plotly.graph_objects.Figure` object.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[43m_imports\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     info_list = _get_optimization_history_info_list(study, target, target_name, error_bar)\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_optimization_history_plot(info_list, target_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Desktop\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\optuna\\_imports.py:97\u001b[39m, in \u001b[36m_DeferredImportExceptionContextManager.check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._deferred \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     96\u001b[39m     exc_value, message = \u001b[38;5;28mself\u001b[39m._deferred\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(message) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc_value\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Tried to import 'plotly' but failed. Please make sure that the package is installed correctly to use this feature. Actual error: No module named 'plotly'."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "optuna.visualization.plot_optimization_history(study).show()\n",
    "optuna.visualization.plot_param_importances(study).show() # 查看哪个参数最重要"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
